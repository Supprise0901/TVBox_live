import random
import requests
from lxml import etree
import os
import threading
import time
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from proxyTest import get_valid_proxies


def get_url(name):
    # proxy = get_valid_proxies()  # å¢åŠ ä»£ç†
    user_agents = [
        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36 Edg/116.0.1938.69',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_3) AppleWebKit/537.36 (KHTML, like Gecko) Version/15.6 Safari/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
        'Mozilla/5.0 (Linux; Android 12; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Mobile Safari/537.36',
        'Mozilla/5.0 (Android 12; Mobile; rv:117.0) Gecko/117.0 Firefox/117.0',
        'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)',
        'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:117.0) Gecko/20100101 Firefox/117.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/116.0.5845.179 Chrome/116.0.5845.179 Safari/537.36',
        'Mozilla/5.0 (compatible; Konqueror/4.14; Linux) KHTML/4.14.2 (like Gecko)',
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Epiphany/42.3 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36 OPR/103.0.4928.47",
    ]
    user_agent = random.choice(user_agents)
    # é…ç½®ChromeOptionsä»¥å¯ç”¨æ— å¤´æ¨¡å¼
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f"user-agent={user_agent}")
    # chrome_options.add_argument(f"--proxy-server={proxy}")  # å¢åŠ ä»£ç†

    # è®¾ç½®ChromeDriver
    driver = webdriver.Chrome(options=chrome_options)

    try:
        # æ‰“å¼€æŒ‡å®šé¡µé¢
        driver.get('http://tonkiang.us/')
        # ç­‰å¾…ç›´åˆ° ID ä¸º 'search' çš„å…ƒç´ å¯è¢«ç‚¹å‡»ï¼ˆæˆ–è€…å¯ä»¥ä¿®æ”¹æˆ visible, presence_of_element_located ç­‰ï¼‰
        username_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, 'search'))
        )
        # username_input = driver.find_element(By.ID, 'search')
        username_input.send_keys(f'{name}')
        submit_button = driver.find_element(By.NAME, 'Submit')
        submit_button.click()
    except Exception as e:
        print(f"æ‰¾ä¸åˆ°å…ƒç´ : {e}")

    try:
        # è·å–é¡µé¢çš„æºä»£ç 
        page_source = driver.page_source
        # æ‰“å°æºä»£ç 
        print(page_source)
        m3u8_list = []
        # å°† HTML è½¬æ¢ä¸º Element å¯¹è±¡
        root = etree.HTML(page_source)
        result_divs = root.xpath("//div[@class='resultplus']")
        print(f"è·å–æ•°æ®: {len(result_divs)}")
        # æ‰“å°æå–åˆ°çš„ <div class="result"> æ ‡ç­¾
        for div in result_divs:
            # å¦‚æœè¦è·å–æ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
            # print(etree.tostring(div, pretty_print=True).decode())
            for element in div.xpath(".//tba"):
                if element.text is not None:
                    # m3u8_list.append(element.text.strip())
                    print(element.text.strip())
                    m3u8_list.append(element.text.strip())
                    with open('m3u8_list.txt', 'a', encoding='utf-8') as f:
                        f.write(f'{name},{element.text.strip()}' + '\n')
    except requests.exceptions.RequestException as e:
        print(f"Error: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        pass

    # å…³é—­WebDriver
    driver.quit()
    return m3u8_list


def download_m3u8(url, name, initial_url=None):
    try:
        # ä¸‹è½½M3U8æ–‡ä»¶
        # with requests.get(url, timeout=10) as response:
        #     response.raise_for_status()
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        m3u8_content = response.text
    except requests.exceptions.Timeout as e:
        print(f"{url}\nError: è¯·æ±‚è¶…æ—¶. Exception: {e}")
        return
    except requests.exceptions.RequestException as e:
        print(f"{url}\nError: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return
    else:
        # è§£æM3U8æ–‡ä»¶ï¼Œè·å–è§†é¢‘ç‰‡æ®µé“¾æ¥
        lines = m3u8_content.split('\n')
        segments = [line.strip() for line in lines if line and not line.startswith('#')]
        if len(segments) == 1:
            # åœ¨é€’å½’è°ƒç”¨æ—¶ä¼ é€’ initial_url å‚æ•°
            return download_m3u8(segments[0], name, initial_url=initial_url if initial_url is not None else url)

        # ä¸‹è½½æŒ‡å®šæ•°é‡çš„è§†é¢‘ç‰‡æ®µå¹¶è®¡ç®—ä¸‹è½½é€Ÿåº¦
        total_size = 0
        total_time = 0
        for i, segment in enumerate(segments[:3]):
            start_time = time.time()
            segment_url = url.rsplit('/', 1)[0] + '/' + segment
            response = requests.get(segment_url)
            end_time = time.time()

            # å°†è§†é¢‘ç‰‡æ®µä¿å­˜åˆ°æœ¬åœ°
            with open('video.ts', 'wb') as f:
                f.write(response.content)

            # è®¡ç®—ä¸‹è½½é€Ÿåº¦
            segment_size = len(response.content)
            segment_time = end_time - start_time
            segment_speed = segment_size / segment_time / (1024 * 1024)  # è½¬æ¢ä¸ºMB/s

            total_size += segment_size
            total_time += segment_time

            print(f"Downloaded segment {i + 1}/3: {segment_speed:.2f} MB/s")

        # è®¡ç®—å¹³å‡ä¸‹è½½é€Ÿåº¦
        average_speed = total_size / total_time / (1024 * 1024)  # è½¬æ¢ä¸ºMB/s
        print(f"---{name}---Average Download Speed: {average_speed:.2f} MB/s")
        # print(f"---{name}---å¹³å‡é€Ÿåº¦: {average_speed:.2f} MB/s")

        # é€Ÿåº¦é˜ˆå€¼ï¼Œé»˜è®¤1MB/s
        if average_speed >= speed:
            valid_url = initial_url if initial_url is not None else url
            if not os.path.exists(f'{TV_name}'):
                os.makedirs(f'{TV_name}')
            with open(os.path.join(f'{TV_name}', f'{name}.txt'), 'a', encoding='utf-8') as file:
                file.write(f'{name},{valid_url}\n')
            print(f"---{name}---é“¾æ¥æœ‰æ•ˆæºå·²ä¿å­˜---\n"
                  f"----{valid_url}---")
            return


def detectLinks(name, m3u8_list):
    thread = []
    for m3u8_url in m3u8_list:
        t = threading.Thread(target=download_m3u8, args=(m3u8_url, name,))
        t.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹,ç¡®ä¿åœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶ï¼Œæ‰€æœ‰å­çº¿ç¨‹éƒ½ä¼šè¢«å¼ºåˆ¶ç»ˆæ­¢
        t.start()
        thread.append(t)
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in thread:
        try:
            print(f"Waiting for thread {t} to finish")
            t.join(timeout=10)  # ç­‰å¾…çº¿ç¨‹è¶…æ—¶
        except Exception as e:
            print(f"Thread {t.name} raised an exception: {e}")


def mer_links(tv):
    # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = [f for f in os.listdir(os.path.join(current_directory, f'{tv}'))]
    print(txt_files)
    # æ‰“å¼€åˆå¹¶åçš„æ–‡ä»¶ï¼Œä½¿ç”¨ 'a' æ¨¡å¼ä»¥è¿½åŠ çš„æ–¹å¼å†™å…¥
    with open(output_file_path, 'a', encoding='utf-8') as output_file:
        output_file.write(f'{tv},#genre#' + '\n')
        for txt_file in txt_files:
            # æ‹¼æ¥æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
            file_path = os.path.join(os.path.join(current_directory, f'{tv}'), txt_file)

            # æ‰“å¼€å½“å‰ txt æ–‡ä»¶å¹¶è¯»å–å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as input_file:
                file_content = input_file.read()

                # å°†å½“å‰ txt æ–‡ä»¶çš„å†…å®¹å†™å…¥åˆå¹¶åçš„æ–‡ä»¶
                output_file.write(file_content)

                # å¯ä»¥é€‰æ‹©åœ¨æ¯ä¸ªæ–‡ä»¶ä¹‹é—´åŠ å…¥æ¢è¡Œï¼Œä½¿åˆå¹¶åçš„å†…å®¹æ›´æ¸…æ™°
                output_file.write('\n')

    print(f'Merged content from {len(txt_files)} files into {output_file_path}')


def re_dup_ordered(filepath):
    from collections import OrderedDict
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    # ä¿æŒåŸå§‹é¡ºåºçš„å»é‡
    unique_lines_ordered = list(OrderedDict.fromkeys(lines))
    # å°†å»é‡åçš„å†…å®¹å†™å›æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as file:
        file.writelines(unique_lines_ordered)
    print('-----ç›´æ’­æºå»é‡å®Œæˆï¼------')


def re_dup(filepath):
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    # è¿‡æ»¤æ‰åŒ…å« 'null' çš„è¡Œ
    filtered_lines = [line for line in lines if 'null' not in line]

    # ä½¿ç”¨å­—å…¸æ¥å»é‡ï¼Œé”®ä¸º URLï¼Œå€¼ä¸ºå®Œæ•´è¡Œå†…å®¹
    unique_lines = {}

    for line in filtered_lines:
        # åˆ†å‰²è¡Œå†…å®¹ï¼Œä»¥è·å– URLï¼ˆå‡è®¾ URL ä½äºè¡Œçš„ç¬¬äºŒéƒ¨åˆ†ï¼‰
        parts = line.strip().split(',')
        if len(parts) == 2:
            channel_name, url = parts[0].strip(), parts[1].strip()
            # å¦‚æœ URL è¿˜æ²¡æœ‰è¢«è®°å½•ï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
            if url not in unique_lines:
                unique_lines[url] = line

    # è·å–å»é‡åçš„è¡Œï¼ˆä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„é¡ºåºï¼‰
    unique_lines_ordered = list(unique_lines.values())

    # å°†å»é‡åçš„å†…å®¹å†™å›æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as file:
        file.writelines(unique_lines_ordered)
    print('-----ç›´æ’­æºå»é‡å®Œæˆï¼------')


if __name__ == '__main__':
    # print('è¯´æ˜ï¼š\n'
    #       'é€Ÿåº¦é˜ˆå€¼é»˜è®¤ä¸º1\n'
    #       'é˜ˆå€¼è¶Šå¤§ï¼Œç›´æ’­æµé€Ÿåº¦è¶Šå¿«ï¼Œæ£€ç´¢å‡ºçš„ç›´æ’­æµæ•°é‡è¶Šå°‘\n'
    #       'å»ºè®®æ—¥å¸¸é˜ˆå€¼æœ€å°0.3ï¼Œèƒ½å¤Ÿæ»¡è¶³æ—¥å¸¸æ’­æ”¾æµä¸å¡é¡¿\n')
    # speed = input('è¯·ç›´æ¥å›è½¦ç¡®å®šæˆ–è¾“å…¥é˜ˆå€¼:  ')
    # if speed == '':
    #     speed = 1
    # else:
    #     speed = float(speed)
    speed = 1
    # è·å–å½“å‰å·¥ä½œç›®å½•
    current_directory = os.getcwd()
    # æ„é€ ä¸Šçº§ç›®å½•çš„è·¯å¾„
    parent_dir = os.path.dirname(current_directory)
    output_file_path = os.path.join(parent_dir, 'live.txt')
    # æ¸…ç©ºlive.txtå†…å®¹
    with open(output_file_path, 'w', encoding='utf-8') as f:
        pass
    with open('m3u8_list.txt', 'w', encoding='utf-8') as file:
        pass
    tv_dict = {}
    # éå†å½“å‰æ–‡ä»¶ä¸‹çš„txtæ–‡ä»¶,æå–æ–‡ä»¶å
    # TV_names = [os.path.splitext(f)[0] for f in os.listdir(current_directory) if f.endswith(".txt")]
    # 'ğŸ‡­ğŸ‡°æ¸¯å°'  'ğŸ‡¨ğŸ‡³å«è§†é¢‘é“'  'ğŸ‡¨ğŸ‡³å¤®è§†é¢‘é“'
    TV_names = ['ğŸ‡¨ğŸ‡³å¤®è§†é¢‘é“']
    for TV_name in TV_names:
        # åˆ é™¤å†å²æµ‹è¯•è®°å½•ï¼Œé˜²æ­¢æ–‡ä»¶è¿½åŠ å†™å…¥
        if os.path.exists(TV_name):
            import shutil

            # åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
            try:
                shutil.rmtree(TV_name)
                print(f"Folder '{TV_name}' deleted successfully.")
            except OSError as e:
                print(f"Error deleting folder '{TV_name}': {e}")
        time.sleep(1)
        if not os.path.exists(TV_name):
            os.makedirs(TV_name)
        # è¯»å–æ–‡ä»¶å¹¶é€è¡Œå¤„ç†ls
        with open(f'{TV_name}.txt', 'r', encoding='utf-8') as file:
            names = [line.strip() for line in file]
            for name in names:
                m3u8_list = get_url(name)
                tv_dict[name] = m3u8_list
                print(name)
            print('---------å­—å…¸åŠ è½½å®Œæˆï¼------------')
        for name, m3u8_list in tv_dict.items():
            detectLinks(name, m3u8_list)
        # åˆå¹¶æœ‰æ•ˆç›´æ’­æºm3u8é“¾æ¥
        mer_links(TV_name)
        tv_dict.clear()

    time.sleep(10)
    os.remove('video.ts')
    # ç›´æ’­æºå»é‡
    # re_dup(output_file_path)
    re_dup_ordered(output_file_path)

    sys.exit()
