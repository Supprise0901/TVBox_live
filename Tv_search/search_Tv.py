import requests
from lxml import etree
import os
import threading
import time
import sys


# def get_url(name):
#     headers = {
#         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
#     }
#     url = "http://tonkiang.us/"
#     # è·å–ä¸¤é¡µçš„m3u8é“¾æ¥
#     # params = {
#     #     "page": 1,
#     #     "s": name
#     # }
#     # response = requests.get(url, headers=headers, params=params, verify=False)
#     data = {
#         "search": name,
#         "Submit": " "
#     }
#     try:
#         time.sleep(5)
#         with requests.Session() as session:
#             response = session.post(url, headers=headers, data=data, verify=False)
#             print(response)
#         # print(response.text)
#         # å°† HTML è½¬æ¢ä¸º Element å¯¹è±¡
#         root = etree.HTML(response.text)
#         result_divs = root.xpath("//div[@class='result']")
#
#         # æ‰“å°æå–åˆ°çš„ <div class="result"> æ ‡ç­¾
#         m3u8_list = []
#         for div in result_divs:
#             # å¦‚æœè¦è·å–æ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
#             # print(etree.tostring(div, pretty_print=True).decode())
#             for element in div.xpath(".//tba"):
#                 if element.text is not None:
#                     m3u8_list.append(element.text.strip())
#                     print(element.text.strip())
#         return m3u8_list
#
#     except requests.exceptions.RequestException as e:
#         print(f"Error: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
#         return
def get_url(name):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }

    url = "http://tonkiang.us/"
    # ç”µè§†å°åå­—æœç´¢
    data = {
        "search": f"{name}",
        "Submit": " "
    }
    try:
        res = requests.get(url, headers=headers, data=data, verify=False)
        cookie = res.cookies
        # æœç´¢é¡µæ•°
        m3u8_list = []
        for i in range(5):
            url = f"http://tonkiang.us/?page={i + 1}&s={name}"
            time.sleep(10)
            response = requests.get(url, headers=headers, cookies=cookie, verify=False)
            # print(response.text)
            # å°† HTML è½¬æ¢ä¸º Element å¯¹è±¡
            root = etree.HTML(response.text)
            result_divs = root.xpath("//div[@class='result']")
            # æ‰“å°æå–åˆ°çš„ <div class="result"> æ ‡ç­¾
            for div in result_divs:
                # å¦‚æœè¦è·å–æ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
                # print(etree.tostring(div, pretty_print=True).decode())
                for element in div.xpath(".//tba"):
                    if element.text is not None:
                        m3u8_list.append(element.text.strip())
                        print(element.text.strip())
        return m3u8_list

    except requests.exceptions.RequestException as e:
        print(f"Error: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        return


def download_m3u8(url, name, initial_url=None):
    try:
        # ä¸‹è½½M3U8æ–‡ä»¶
        # with requests.get(url, timeout=10) as response:
        #     response.raise_for_status()
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        m3u8_content = response.text
    except requests.exceptions.Timeout as e:
        print(f"{url}\nError: è¯·æ±‚è¶…æ—¶. Exception: {e}")
        return
    except requests.exceptions.RequestException as e:
        print(f"{url}\nError: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return
    else:
        # è§£æM3U8æ–‡ä»¶ï¼Œè·å–è§†é¢‘ç‰‡æ®µé“¾æ¥
        lines = m3u8_content.split('\n')
        segments = [line.strip() for line in lines if line and not line.startswith('#')]
        if len(segments) == 1:
            # åœ¨é€’å½’è°ƒç”¨æ—¶ä¼ é€’ initial_url å‚æ•°
            return download_m3u8(segments[0], name, initial_url=initial_url if initial_url is not None else url)

        # ä¸‹è½½æŒ‡å®šæ•°é‡çš„è§†é¢‘ç‰‡æ®µå¹¶è®¡ç®—ä¸‹è½½é€Ÿåº¦
        total_size = 0
        total_time = 0
        for i, segment in enumerate(segments[:3]):
            start_time = time.time()
            segment_url = url.rsplit('/', 1)[0] + '/' + segment
            response = requests.get(segment_url)
            end_time = time.time()

            # å°†è§†é¢‘ç‰‡æ®µä¿å­˜åˆ°æœ¬åœ°
            with open('video.ts', 'wb') as f:
                f.write(response.content)

            # è®¡ç®—ä¸‹è½½é€Ÿåº¦
            segment_size = len(response.content)
            segment_time = end_time - start_time
            segment_speed = segment_size / segment_time / (1024 * 1024)  # è½¬æ¢ä¸ºMB/s

            total_size += segment_size
            total_time += segment_time

            print(f"Downloaded segment {i + 1}/3: {segment_speed:.2f} MB/s")

        # è®¡ç®—å¹³å‡ä¸‹è½½é€Ÿåº¦
        average_speed = total_size / total_time / (1024 * 1024)  # è½¬æ¢ä¸ºMB/s
        print(f"---{name}---Average Download Speed: {average_speed:.2f} MB/s")
        # print(f"---{name}---å¹³å‡é€Ÿåº¦: {average_speed:.2f} MB/s")

        # é€Ÿåº¦é˜ˆå€¼ï¼Œé»˜è®¤1MB/s
        if average_speed >= speed:
            valid_url = initial_url if initial_url is not None else url
            if not os.path.exists(f'{TV_name}'):
                os.makedirs(f'{TV_name}')
            with open(os.path.join(f'{TV_name}', f'{name}.txt'), 'a', encoding='utf-8') as file:
                file.write(f'{name},{valid_url}\n')
            print(f"---{name}---é“¾æ¥æœ‰æ•ˆæºå·²ä¿å­˜---\n"
                  f"----{valid_url}---")
            return


def detectLinks(name, m3u8_list):
    thread = []
    for m3u8_url in m3u8_list:
        t = threading.Thread(target=download_m3u8, args=(m3u8_url, name,))
        t.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹,ç¡®ä¿åœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶ï¼Œæ‰€æœ‰å­çº¿ç¨‹éƒ½ä¼šè¢«å¼ºåˆ¶ç»ˆæ­¢
        t.start()
        thread.append(t)
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in thread:
        try:
            print(f"Waiting for thread {t} to finish")
            t.join(timeout=10)  # ç­‰å¾…çº¿ç¨‹è¶…æ—¶
        except Exception as e:
            print(f"Thread {t.name} raised an exception: {e}")


def mer_links(tv):
    # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = [f for f in os.listdir(os.path.join(current_directory, f'{tv}'))]
    print(txt_files)
    # æ‰“å¼€åˆå¹¶åçš„æ–‡ä»¶ï¼Œä½¿ç”¨ 'a' æ¨¡å¼ä»¥è¿½åŠ çš„æ–¹å¼å†™å…¥
    with open(output_file_path, 'a', encoding='utf-8') as output_file:
        output_file.write(f'{tv},#genre#' + '\n')
        for txt_file in txt_files:
            # æ‹¼æ¥æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
            file_path = os.path.join(os.path.join(current_directory, f'{tv}'), txt_file)

            # æ‰“å¼€å½“å‰ txt æ–‡ä»¶å¹¶è¯»å–å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as input_file:
                file_content = input_file.read()

                # å°†å½“å‰ txt æ–‡ä»¶çš„å†…å®¹å†™å…¥åˆå¹¶åçš„æ–‡ä»¶
                output_file.write(file_content)

                # å¯ä»¥é€‰æ‹©åœ¨æ¯ä¸ªæ–‡ä»¶ä¹‹é—´åŠ å…¥æ¢è¡Œï¼Œä½¿åˆå¹¶åçš„å†…å®¹æ›´æ¸…æ™°
                output_file.write('\n')

    print(f'Merged content from {len(txt_files)} files into {output_file_path}')


def re_dup(filepath):
    from collections import OrderedDict
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    # ä¿æŒåŸå§‹é¡ºåºçš„å»é‡
    unique_lines_ordered = list(OrderedDict.fromkeys(lines))
    # å°†å»é‡åçš„å†…å®¹å†™å›æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as file:
        file.writelines(unique_lines_ordered)
    print('-----ç›´æ’­æºå»é‡å®Œæˆï¼------')


if __name__ == '__main__':
    print('è¯´æ˜ï¼š\n'
          'é€Ÿåº¦é˜ˆå€¼é»˜è®¤ä¸º1\n'
          'é˜ˆå€¼è¶Šå¤§ï¼Œç›´æ’­æµé€Ÿåº¦è¶Šå¿«ï¼Œæ£€ç´¢å‡ºçš„ç›´æ’­æµæ•°é‡è¶Šå°‘\n'
          'å»ºè®®æ—¥å¸¸é˜ˆå€¼æœ€å°0.3ï¼Œèƒ½å¤Ÿæ»¡è¶³æ—¥å¸¸æ’­æ”¾æµä¸å¡é¡¿\n')
    # speed = input('è¯·ç›´æ¥å›è½¦ç¡®å®šæˆ–è¾“å…¥é˜ˆå€¼:  ')
    # if speed == '':
    #     speed = 1
    # else:
    #     speed = float(speed)
    speed = 1
    # è·å–å½“å‰å·¥ä½œç›®å½•
    current_directory = os.getcwd()
    # æ„é€ ä¸Šçº§ç›®å½•çš„è·¯å¾„
    parent_dir = os.path.dirname(current_directory)
    output_file_path = os.path.join(parent_dir, 'live.txt')
    # æ¸…ç©ºlive.txtå†…å®¹
    with open(output_file_path, 'w', encoding='utf-8') as f:
        pass
    tv_dict = {}
    # éå†å½“å‰æ–‡ä»¶ä¸‹çš„txtæ–‡ä»¶,æå–æ–‡ä»¶å
    TV_names = [os.path.splitext(f)[0] for f in os.listdir(current_directory) if f.endswith(".txt")]
    # 'ğŸ‡­ğŸ‡°æ¸¯å°'  'ğŸ‡¨ğŸ‡³å«è§†é¢‘é“'  'ğŸ‡¨ğŸ‡³å¤®è§†é¢‘é“'
    # TV_names = ['ğŸ‡¨ğŸ‡³å«è§†é¢‘é“']
    for TV_name in TV_names:
        # åˆ é™¤å†å²æµ‹è¯•è®°å½•ï¼Œé˜²æ­¢æ–‡ä»¶è¿½åŠ å†™å…¥
        if os.path.exists(TV_name):
            import shutil
            # åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶å†…å®¹
            try:
                shutil.rmtree(TV_name)
                print(f"Folder '{TV_name}' deleted successfully.")
            except OSError as e:
                print(f"Error deleting folder '{TV_name}': {e}")
        time.sleep(1)
        if not os.path.exists(TV_name):
            os.makedirs(TV_name)
        # è¯»å–æ–‡ä»¶å¹¶é€è¡Œå¤„ç†
        with open(f'{TV_name}.txt', 'r', encoding='utf-8') as file:
            names = [line.strip() for line in file]
            for name in names:
                m3u8_list = get_url(name)
                tv_dict[name] = m3u8_list
                print(name)
            print('---------å­—å…¸åŠ è½½å®Œæˆï¼------------')
        for name, m3u8_list in tv_dict.items():
            detectLinks(name, m3u8_list)
        # åˆå¹¶æœ‰æ•ˆç›´æ’­æºm3u8é“¾æ¥
        mer_links(TV_name)
        tv_dict.clear()

    time.sleep(10)
    os.remove('video.ts')
    # ç›´æ’­æºå»é‡
    re_dup(output_file_path)

    sys.exit()
